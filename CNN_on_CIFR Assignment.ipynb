{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHCYMwwXflQd"
   },
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use DropOut layers.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:35:26.172165Z",
     "iopub.status.busy": "2022-01-08T14:35:26.171580Z",
     "iopub.status.idle": "2022-01-08T14:35:30.906219Z",
     "shell.execute_reply": "2022-01-08T14:35:30.905479Z",
     "shell.execute_reply.started": "2022-01-08T14:35:26.172070Z"
    },
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:35:30.933944Z",
     "iopub.status.busy": "2022-01-08T14:35:30.933466Z",
     "iopub.status.idle": "2022-01-08T14:35:39.194845Z",
     "shell.execute_reply": "2022-01-08T14:35:39.194099Z",
     "shell.execute_reply.started": "2022-01-08T14:35:30.933908Z"
    },
    "id": "mB7o3zu1g6eT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n",
      "170508288/170498071 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:35:39.197361Z",
     "iopub.status.busy": "2022-01-08T14:35:39.197114Z",
     "iopub.status.idle": "2022-01-08T14:35:39.384487Z",
     "shell.execute_reply": "2022-01-08T14:35:39.383662Z",
     "shell.execute_reply.started": "2022-01-08T14:35:39.197328Z"
    },
    "id": "qhG-KCHt-9Ps"
   },
   "outputs": [],
   "source": [
    "#converting to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:35:39.386233Z",
     "iopub.status.busy": "2022-01-08T14:35:39.385966Z",
     "iopub.status.idle": "2022-01-08T14:35:39.394385Z",
     "shell.execute_reply": "2022-01-08T14:35:39.393420Z",
     "shell.execute_reply.started": "2022-01-08T14:35:39.386197Z"
    },
    "id": "3lAk_Mw_5-rn",
    "outputId": "deffb988-6f98-4167-ed09-e570e5bc917a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:35:39.396526Z",
     "iopub.status.busy": "2022-01-08T14:35:39.396004Z",
     "iopub.status.idle": "2022-01-08T14:35:39.405471Z",
     "shell.execute_reply": "2022-01-08T14:35:39.404392Z",
     "shell.execute_reply.started": "2022-01-08T14:35:39.396464Z"
    },
    "id": "DVkpgHsc5-rp",
    "outputId": "2db7b9f0-bee8-4727-ead2-074973d55e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:35:39.407883Z",
     "iopub.status.busy": "2022-01-08T14:35:39.407576Z",
     "iopub.status.idle": "2022-01-08T14:35:39.416679Z",
     "shell.execute_reply": "2022-01-08T14:35:39.415832Z",
     "shell.execute_reply.started": "2022-01-08T14:35:39.407844Z"
    },
    "id": "EORwct12v60C"
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen_train = ImageDataGenerator(rescale=1./255,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:35:39.418722Z",
     "iopub.status.busy": "2022-01-08T14:35:39.418368Z",
     "iopub.status.idle": "2022-01-08T14:35:39.433972Z",
     "shell.execute_reply": "2022-01-08T14:35:39.433055Z",
     "shell.execute_reply.started": "2022-01-08T14:35:39.418684Z"
    },
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T14:54:43.809001Z",
     "iopub.status.busy": "2022-01-08T14:54:43.808737Z",
     "iopub.status.idle": "2022-01-08T14:54:43.813750Z",
     "shell.execute_reply": "2022-01-08T14:54:43.812737Z",
     "shell.execute_reply.started": "2022-01-08T14:54:43.808972Z"
    },
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 300\n",
    "l = 12\n",
    "num_filter = 35\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:05:49.918348Z",
     "iopub.status.busy": "2022-01-08T15:05:49.918081Z",
     "iopub.status.idle": "2022-01-08T15:05:51.728780Z",
     "shell.execute_reply": "2022-01-08T15:05:51.727945Z",
     "shell.execute_reply.started": "2022-01-08T15:05:49.918316Z"
    },
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "#defining the model\n",
    "\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:05:51.731710Z",
     "iopub.status.busy": "2022-01-08T15:05:51.731290Z",
     "iopub.status.idle": "2022-01-08T15:05:51.858241Z",
     "shell.execute_reply": "2022-01-08T15:05:51.856519Z",
     "shell.execute_reply.started": "2022-01-08T15:05:51.731671Z"
    },
    "id": "1kFh7pdxhNtT",
    "outputId": "984d6467-c971-42c6-a3dd-c2bdf4cdc71b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 32, 32, 35)   945         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 32, 32, 35)   140         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 32, 32, 35)   0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 32, 32, 17)   5355        activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_204 (Dropout)           (None, 32, 32, 17)   0           conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_192 (Concatenate)   (None, 32, 32, 52)   0           conv2d_208[0][0]                 \n",
      "                                                                 dropout_204[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 32, 32, 52)   208         concatenate_192[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 32, 32, 52)   0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 32, 32, 17)   7956        activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_205 (Dropout)           (None, 32, 32, 17)   0           conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_193 (Concatenate)   (None, 32, 32, 69)   0           concatenate_192[0][0]            \n",
      "                                                                 dropout_205[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 32, 32, 69)   276         concatenate_193[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 32, 32, 69)   0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 32, 32, 17)   10557       activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_206 (Dropout)           (None, 32, 32, 17)   0           conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_194 (Concatenate)   (None, 32, 32, 86)   0           concatenate_193[0][0]            \n",
      "                                                                 dropout_206[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 32, 32, 86)   344         concatenate_194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 32, 32, 86)   0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 32, 32, 17)   13158       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_207 (Dropout)           (None, 32, 32, 17)   0           conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_195 (Concatenate)   (None, 32, 32, 103)  0           concatenate_194[0][0]            \n",
      "                                                                 dropout_207[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 32, 32, 103)  412         concatenate_195[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 32, 32, 103)  0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 32, 32, 17)   15759       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_208 (Dropout)           (None, 32, 32, 17)   0           conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_196 (Concatenate)   (None, 32, 32, 120)  0           concatenate_195[0][0]            \n",
      "                                                                 dropout_208[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 32, 32, 120)  480         concatenate_196[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 32, 32, 120)  0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 32, 32, 17)   18360       activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_209 (Dropout)           (None, 32, 32, 17)   0           conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_197 (Concatenate)   (None, 32, 32, 137)  0           concatenate_196[0][0]            \n",
      "                                                                 dropout_209[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 32, 32, 137)  548         concatenate_197[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 32, 32, 137)  0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 32, 32, 17)   20961       activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_210 (Dropout)           (None, 32, 32, 17)   0           conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_198 (Concatenate)   (None, 32, 32, 154)  0           concatenate_197[0][0]            \n",
      "                                                                 dropout_210[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 32, 32, 154)  616         concatenate_198[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 32, 32, 154)  0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 32, 32, 17)   23562       activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_211 (Dropout)           (None, 32, 32, 17)   0           conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_199 (Concatenate)   (None, 32, 32, 171)  0           concatenate_198[0][0]            \n",
      "                                                                 dropout_211[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 32, 32, 171)  684         concatenate_199[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 32, 32, 171)  0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 32, 32, 17)   26163       activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_212 (Dropout)           (None, 32, 32, 17)   0           conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_200 (Concatenate)   (None, 32, 32, 188)  0           concatenate_199[0][0]            \n",
      "                                                                 dropout_212[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 32, 32, 188)  752         concatenate_200[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 32, 32, 188)  0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 32, 32, 17)   28764       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_213 (Dropout)           (None, 32, 32, 17)   0           conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_201 (Concatenate)   (None, 32, 32, 205)  0           concatenate_200[0][0]            \n",
      "                                                                 dropout_213[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 32, 32, 205)  820         concatenate_201[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 32, 32, 205)  0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 32, 32, 17)   31365       activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_214 (Dropout)           (None, 32, 32, 17)   0           conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_202 (Concatenate)   (None, 32, 32, 222)  0           concatenate_201[0][0]            \n",
      "                                                                 dropout_214[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 32, 32, 222)  888         concatenate_202[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 32, 32, 222)  0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 32, 32, 17)   33966       activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_215 (Dropout)           (None, 32, 32, 17)   0           conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_203 (Concatenate)   (None, 32, 32, 239)  0           concatenate_202[0][0]            \n",
      "                                                                 dropout_215[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 32, 32, 239)  956         concatenate_203[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 32, 32, 239)  0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 32, 32, 17)   4063        activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_216 (Dropout)           (None, 32, 32, 17)   0           conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 16, 16, 17)   0           dropout_216[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 16, 16, 17)   68          average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 16, 16, 17)   0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 16, 16, 17)   2601        activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_217 (Dropout)           (None, 16, 16, 17)   0           conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_204 (Concatenate)   (None, 16, 16, 34)   0           average_pooling2d_16[0][0]       \n",
      "                                                                 dropout_217[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 16, 16, 34)   136         concatenate_204[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 16, 16, 34)   0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 16, 16, 17)   5202        activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_218 (Dropout)           (None, 16, 16, 17)   0           conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_205 (Concatenate)   (None, 16, 16, 51)   0           concatenate_204[0][0]            \n",
      "                                                                 dropout_218[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 16, 16, 51)   204         concatenate_205[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 16, 16, 51)   0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 16, 16, 17)   7803        activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_219 (Dropout)           (None, 16, 16, 17)   0           conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_206 (Concatenate)   (None, 16, 16, 68)   0           concatenate_205[0][0]            \n",
      "                                                                 dropout_219[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 16, 16, 68)   272         concatenate_206[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 16, 16, 68)   0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 16, 16, 17)   10404       activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_220 (Dropout)           (None, 16, 16, 17)   0           conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_207 (Concatenate)   (None, 16, 16, 85)   0           concatenate_206[0][0]            \n",
      "                                                                 dropout_220[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 16, 16, 85)   340         concatenate_207[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 16, 16, 85)   0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 16, 16, 17)   13005       activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_221 (Dropout)           (None, 16, 16, 17)   0           conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_208 (Concatenate)   (None, 16, 16, 102)  0           concatenate_207[0][0]            \n",
      "                                                                 dropout_221[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 16, 16, 102)  408         concatenate_208[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 16, 16, 102)  0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 16, 16, 17)   15606       activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_222 (Dropout)           (None, 16, 16, 17)   0           conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_209 (Concatenate)   (None, 16, 16, 119)  0           concatenate_208[0][0]            \n",
      "                                                                 dropout_222[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 16, 16, 119)  476         concatenate_209[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 16, 16, 119)  0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 16, 16, 17)   18207       activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_223 (Dropout)           (None, 16, 16, 17)   0           conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_210 (Concatenate)   (None, 16, 16, 136)  0           concatenate_209[0][0]            \n",
      "                                                                 dropout_223[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 16, 16, 136)  544         concatenate_210[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 16, 16, 136)  0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 16, 16, 17)   20808       activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_224 (Dropout)           (None, 16, 16, 17)   0           conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_211 (Concatenate)   (None, 16, 16, 153)  0           concatenate_210[0][0]            \n",
      "                                                                 dropout_224[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 16, 16, 153)  612         concatenate_211[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 16, 16, 153)  0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 16, 16, 17)   23409       activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_225 (Dropout)           (None, 16, 16, 17)   0           conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_212 (Concatenate)   (None, 16, 16, 170)  0           concatenate_211[0][0]            \n",
      "                                                                 dropout_225[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 16, 16, 170)  680         concatenate_212[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 16, 16, 170)  0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 16, 16, 17)   26010       activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_226 (Dropout)           (None, 16, 16, 17)   0           conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_213 (Concatenate)   (None, 16, 16, 187)  0           concatenate_212[0][0]            \n",
      "                                                                 dropout_226[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 16, 16, 187)  748         concatenate_213[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 16, 16, 187)  0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 16, 16, 17)   28611       activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_227 (Dropout)           (None, 16, 16, 17)   0           conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_214 (Concatenate)   (None, 16, 16, 204)  0           concatenate_213[0][0]            \n",
      "                                                                 dropout_227[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 16, 16, 204)  816         concatenate_214[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 16, 16, 204)  0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 16, 16, 17)   31212       activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_228 (Dropout)           (None, 16, 16, 17)   0           conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_215 (Concatenate)   (None, 16, 16, 221)  0           concatenate_214[0][0]            \n",
      "                                                                 dropout_228[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 16, 16, 221)  884         concatenate_215[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 16, 16, 221)  0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 16, 16, 17)   3757        activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_229 (Dropout)           (None, 16, 16, 17)   0           conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 8, 8, 17)     0           dropout_229[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 8, 8, 17)     68          average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 8, 8, 17)     0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 8, 8, 17)     2601        activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_230 (Dropout)           (None, 8, 8, 17)     0           conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_216 (Concatenate)   (None, 8, 8, 34)     0           average_pooling2d_17[0][0]       \n",
      "                                                                 dropout_230[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 8, 8, 34)     136         concatenate_216[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 8, 8, 34)     0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 8, 8, 17)     5202        activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_231 (Dropout)           (None, 8, 8, 17)     0           conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_217 (Concatenate)   (None, 8, 8, 51)     0           concatenate_216[0][0]            \n",
      "                                                                 dropout_231[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 8, 8, 51)     204         concatenate_217[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 8, 8, 51)     0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 8, 8, 17)     7803        activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_232 (Dropout)           (None, 8, 8, 17)     0           conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_218 (Concatenate)   (None, 8, 8, 68)     0           concatenate_217[0][0]            \n",
      "                                                                 dropout_232[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 8, 8, 68)     272         concatenate_218[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 8, 8, 68)     0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 8, 8, 17)     10404       activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_233 (Dropout)           (None, 8, 8, 17)     0           conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_219 (Concatenate)   (None, 8, 8, 85)     0           concatenate_218[0][0]            \n",
      "                                                                 dropout_233[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 8, 8, 85)     340         concatenate_219[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 8, 8, 85)     0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 8, 8, 17)     13005       activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_234 (Dropout)           (None, 8, 8, 17)     0           conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_220 (Concatenate)   (None, 8, 8, 102)    0           concatenate_219[0][0]            \n",
      "                                                                 dropout_234[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 8, 8, 102)    408         concatenate_220[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 8, 8, 102)    0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 8, 8, 17)     15606       activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_235 (Dropout)           (None, 8, 8, 17)     0           conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_221 (Concatenate)   (None, 8, 8, 119)    0           concatenate_220[0][0]            \n",
      "                                                                 dropout_235[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 8, 8, 119)    476         concatenate_221[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 8, 8, 119)    0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 8, 8, 17)     18207       activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_236 (Dropout)           (None, 8, 8, 17)     0           conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_222 (Concatenate)   (None, 8, 8, 136)    0           concatenate_221[0][0]            \n",
      "                                                                 dropout_236[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 8, 8, 136)    544         concatenate_222[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 8, 8, 136)    0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 8, 8, 17)     20808       activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_237 (Dropout)           (None, 8, 8, 17)     0           conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_223 (Concatenate)   (None, 8, 8, 153)    0           concatenate_222[0][0]            \n",
      "                                                                 dropout_237[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 8, 8, 153)    612         concatenate_223[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 8, 8, 153)    0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 8, 8, 17)     23409       activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_238 (Dropout)           (None, 8, 8, 17)     0           conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_224 (Concatenate)   (None, 8, 8, 170)    0           concatenate_223[0][0]            \n",
      "                                                                 dropout_238[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 8, 8, 170)    680         concatenate_224[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 8, 8, 170)    0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 8, 8, 17)     26010       activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_239 (Dropout)           (None, 8, 8, 17)     0           conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_225 (Concatenate)   (None, 8, 8, 187)    0           concatenate_224[0][0]            \n",
      "                                                                 dropout_239[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 8, 8, 187)    748         concatenate_225[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 8, 8, 187)    0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 8, 8, 17)     28611       activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_240 (Dropout)           (None, 8, 8, 17)     0           conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_226 (Concatenate)   (None, 8, 8, 204)    0           concatenate_225[0][0]            \n",
      "                                                                 dropout_240[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 8, 8, 204)    816         concatenate_226[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 8, 8, 204)    0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 8, 8, 17)     31212       activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_241 (Dropout)           (None, 8, 8, 17)     0           conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_227 (Concatenate)   (None, 8, 8, 221)    0           concatenate_226[0][0]            \n",
      "                                                                 dropout_241[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, 8, 8, 221)    884         concatenate_227[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 8, 8, 221)    0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 8, 8, 17)     3757        activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_242 (Dropout)           (None, 8, 8, 17)     0           conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 4, 4, 17)     0           dropout_242[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 4, 4, 17)     68          average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 4, 4, 17)     0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 4, 4, 17)     2601        activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_243 (Dropout)           (None, 4, 4, 17)     0           conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_228 (Concatenate)   (None, 4, 4, 34)     0           average_pooling2d_18[0][0]       \n",
      "                                                                 dropout_243[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 4, 4, 34)     136         concatenate_228[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 4, 4, 34)     0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)             (None, 4, 4, 17)     5202        activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_244 (Dropout)           (None, 4, 4, 17)     0           conv2d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_229 (Concatenate)   (None, 4, 4, 51)     0           concatenate_228[0][0]            \n",
      "                                                                 dropout_244[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchN (None, 4, 4, 51)     204         concatenate_229[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 4, 4, 51)     0           batch_normalization_249[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, 4, 4, 17)     7803        activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_245 (Dropout)           (None, 4, 4, 17)     0           conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_230 (Concatenate)   (None, 4, 4, 68)     0           concatenate_229[0][0]            \n",
      "                                                                 dropout_245[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, 4, 4, 68)     272         concatenate_230[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 4, 4, 68)     0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, 4, 4, 17)     10404       activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_246 (Dropout)           (None, 4, 4, 17)     0           conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_231 (Concatenate)   (None, 4, 4, 85)     0           concatenate_230[0][0]            \n",
      "                                                                 dropout_246[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, 4, 4, 85)     340         concatenate_231[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 4, 4, 85)     0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)             (None, 4, 4, 17)     13005       activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_247 (Dropout)           (None, 4, 4, 17)     0           conv2d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_232 (Concatenate)   (None, 4, 4, 102)    0           concatenate_231[0][0]            \n",
      "                                                                 dropout_247[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 4, 4, 102)    408         concatenate_232[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 4, 4, 102)    0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, 4, 4, 17)     15606       activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_248 (Dropout)           (None, 4, 4, 17)     0           conv2d_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_233 (Concatenate)   (None, 4, 4, 119)    0           concatenate_232[0][0]            \n",
      "                                                                 dropout_248[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 4, 4, 119)    476         concatenate_233[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 4, 4, 119)    0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, 4, 4, 17)     18207       activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_249 (Dropout)           (None, 4, 4, 17)     0           conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_234 (Concatenate)   (None, 4, 4, 136)    0           concatenate_233[0][0]            \n",
      "                                                                 dropout_249[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 4, 4, 136)    544         concatenate_234[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 4, 4, 136)    0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)             (None, 4, 4, 17)     20808       activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_250 (Dropout)           (None, 4, 4, 17)     0           conv2d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_235 (Concatenate)   (None, 4, 4, 153)    0           concatenate_234[0][0]            \n",
      "                                                                 dropout_250[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, 4, 4, 153)    612         concatenate_235[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 4, 4, 153)    0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, 4, 4, 17)     23409       activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_251 (Dropout)           (None, 4, 4, 17)     0           conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_236 (Concatenate)   (None, 4, 4, 170)    0           concatenate_235[0][0]            \n",
      "                                                                 dropout_251[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 4, 4, 170)    680         concatenate_236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 4, 4, 170)    0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, 4, 4, 17)     26010       activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_252 (Dropout)           (None, 4, 4, 17)     0           conv2d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_237 (Concatenate)   (None, 4, 4, 187)    0           concatenate_236[0][0]            \n",
      "                                                                 dropout_252[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, 4, 4, 187)    748         concatenate_237[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 4, 4, 187)    0           batch_normalization_257[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, 4, 4, 17)     28611       activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_253 (Dropout)           (None, 4, 4, 17)     0           conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_238 (Concatenate)   (None, 4, 4, 204)    0           concatenate_237[0][0]            \n",
      "                                                                 dropout_253[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, 4, 4, 204)    816         concatenate_238[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 4, 4, 204)    0           batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)             (None, 4, 4, 17)     31212       activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_254 (Dropout)           (None, 4, 4, 17)     0           conv2d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_239 (Concatenate)   (None, 4, 4, 221)    0           concatenate_238[0][0]            \n",
      "                                                                 dropout_254[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchN (None, 4, 4, 221)    884         concatenate_239[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 4, 4, 221)    0           batch_normalization_259[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 2, 2, 221)    0           activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 884)          0           average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           8850        flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 891,620\n",
      "Trainable params: 878,776\n",
      "Non-trainable params: 12,844\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "\n",
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:05:51.859680Z",
     "iopub.status.busy": "2022-01-08T15:05:51.859435Z",
     "iopub.status.idle": "2022-01-08T15:05:51.864335Z",
     "shell.execute_reply": "2022-01-08T15:05:51.863663Z",
     "shell.execute_reply.started": "2022-01-08T15:05:51.859650Z"
    },
    "id": "8Aqzk9AFXb1y",
    "outputId": "f4f33d6a-b280-42e9-8a45-db3ad647ed11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "#number of layers in the model\n",
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:05:51.866142Z",
     "iopub.status.busy": "2022-01-08T15:05:51.865894Z",
     "iopub.status.idle": "2022-01-08T15:05:51.881973Z",
     "shell.execute_reply": "2022-01-08T15:05:51.881143Z",
     "shell.execute_reply.started": "2022-01-08T15:05:51.866109Z"
    },
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:05:56.353042Z",
     "iopub.status.busy": "2022-01-08T15:05:56.352744Z",
     "iopub.status.idle": "2022-01-08T15:05:56.358424Z",
     "shell.execute_reply": "2022-01-08T15:05:56.357517Z",
     "shell.execute_reply.started": "2022-01-08T15:05:56.353008Z"
    },
    "id": "_2OCfp66mwJ4"
   },
   "outputs": [],
   "source": [
    "#callback class to monitor validation accuracy\n",
    "class Monitor(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs['val_accuracy']>0.90):\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:08:06.342373Z",
     "iopub.status.busy": "2022-01-08T15:08:06.341988Z",
     "iopub.status.idle": "2022-01-08T17:08:56.861200Z",
     "shell.execute_reply": "2022-01-08T17:08:56.860490Z",
     "shell.execute_reply.started": "2022-01-08T15:08:06.342315Z"
    },
    "id": "bW0X-0NjykX6",
    "outputId": "8bc19eb3-6446-44d1-fb7e-969d60cfc1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "390/390 [==============================] - 61s 141ms/step - loss: 1.6204 - accuracy: 0.4011 - val_loss: 1.8438 - val_accuracy: 0.3532\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.35320, saving model to model_save/best_model.hdf5\n",
      "Epoch 2/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 1.2690 - accuracy: 0.5411 - val_loss: 1.4784 - val_accuracy: 0.5341\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.35320 to 0.53410, saving model to model_save/best_model.hdf5\n",
      "Epoch 3/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 1.0827 - accuracy: 0.6112 - val_loss: 1.4225 - val_accuracy: 0.5692\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.53410 to 0.56920, saving model to model_save/best_model.hdf5\n",
      "Epoch 4/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.9655 - accuracy: 0.6567 - val_loss: 1.5663 - val_accuracy: 0.5750\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.56920 to 0.57500, saving model to model_save/best_model.hdf5\n",
      "Epoch 5/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.8789 - accuracy: 0.6904 - val_loss: 1.1509 - val_accuracy: 0.6390\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.57500 to 0.63900, saving model to model_save/best_model.hdf5\n",
      "Epoch 6/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.8103 - accuracy: 0.7150 - val_loss: 1.1642 - val_accuracy: 0.6719\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.63900 to 0.67190, saving model to model_save/best_model.hdf5\n",
      "Epoch 7/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.7565 - accuracy: 0.7346 - val_loss: 0.8054 - val_accuracy: 0.7512\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.67190 to 0.75120, saving model to model_save/best_model.hdf5\n",
      "Epoch 8/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.7155 - accuracy: 0.7507 - val_loss: 1.0965 - val_accuracy: 0.6964\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.75120\n",
      "Epoch 9/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.6713 - accuracy: 0.7653 - val_loss: 0.9052 - val_accuracy: 0.7368\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.75120\n",
      "Epoch 10/300\n",
      "390/390 [==============================] - 54s 138ms/step - loss: 0.6516 - accuracy: 0.7727 - val_loss: 1.3157 - val_accuracy: 0.6731\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.75120\n",
      "Epoch 11/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.6241 - accuracy: 0.7841 - val_loss: 1.0696 - val_accuracy: 0.7124\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.75120\n",
      "Epoch 12/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.5997 - accuracy: 0.7930 - val_loss: 1.3314 - val_accuracy: 0.6747\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.75120\n",
      "Epoch 13/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.5822 - accuracy: 0.7989 - val_loss: 0.8621 - val_accuracy: 0.7548\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.75120 to 0.75480, saving model to model_save/best_model.hdf5\n",
      "Epoch 14/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.5607 - accuracy: 0.8060 - val_loss: 0.7631 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.75480 to 0.78060, saving model to model_save/best_model.hdf5\n",
      "Epoch 15/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.5393 - accuracy: 0.8123 - val_loss: 1.4377 - val_accuracy: 0.6638\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.78060\n",
      "Epoch 16/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.5279 - accuracy: 0.8148 - val_loss: 1.0017 - val_accuracy: 0.7353\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.78060\n",
      "Epoch 17/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.5165 - accuracy: 0.8204 - val_loss: 1.1113 - val_accuracy: 0.7300\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.78060\n",
      "Epoch 18/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.5066 - accuracy: 0.8238 - val_loss: 1.0552 - val_accuracy: 0.7166\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.78060\n",
      "Epoch 19/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.4905 - accuracy: 0.8303 - val_loss: 0.7826 - val_accuracy: 0.7896\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.78060 to 0.78960, saving model to model_save/best_model.hdf5\n",
      "Epoch 20/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.4761 - accuracy: 0.8339 - val_loss: 1.2849 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.78960\n",
      "Epoch 21/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.4613 - accuracy: 0.8402 - val_loss: 0.7791 - val_accuracy: 0.7899\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.78960 to 0.78990, saving model to model_save/best_model.hdf5\n",
      "Epoch 22/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.4604 - accuracy: 0.8413 - val_loss: 1.4122 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.78990\n",
      "Epoch 23/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.4504 - accuracy: 0.8444 - val_loss: 0.6031 - val_accuracy: 0.8204\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.78990 to 0.82040, saving model to model_save/best_model.hdf5\n",
      "Epoch 24/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.4415 - accuracy: 0.8463 - val_loss: 0.8195 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.82040\n",
      "Epoch 25/300\n",
      "390/390 [==============================] - 54s 138ms/step - loss: 0.4363 - accuracy: 0.8492 - val_loss: 0.5776 - val_accuracy: 0.8419\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.82040 to 0.84190, saving model to model_save/best_model.hdf5\n",
      "Epoch 26/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.4225 - accuracy: 0.8525 - val_loss: 1.1978 - val_accuracy: 0.7339\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.84190\n",
      "Epoch 27/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.4166 - accuracy: 0.8559 - val_loss: 1.2445 - val_accuracy: 0.7152\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.84190\n",
      "Epoch 28/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.4052 - accuracy: 0.8587 - val_loss: 0.6883 - val_accuracy: 0.8230\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.84190\n",
      "Epoch 29/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.4037 - accuracy: 0.8595 - val_loss: 0.7572 - val_accuracy: 0.8059\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.84190\n",
      "Epoch 30/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3893 - accuracy: 0.8640 - val_loss: 0.5679 - val_accuracy: 0.8454\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.84190 to 0.84540, saving model to model_save/best_model.hdf5\n",
      "Epoch 31/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3894 - accuracy: 0.8663 - val_loss: 0.7087 - val_accuracy: 0.8192\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.84540\n",
      "Epoch 32/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3829 - accuracy: 0.8683 - val_loss: 0.5645 - val_accuracy: 0.8435\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.84540\n",
      "Epoch 33/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3747 - accuracy: 0.8705 - val_loss: 0.5997 - val_accuracy: 0.8426\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.84540\n",
      "Epoch 34/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3736 - accuracy: 0.8700 - val_loss: 0.7671 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.84540\n",
      "Epoch 35/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3622 - accuracy: 0.8737 - val_loss: 0.9533 - val_accuracy: 0.7765\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.84540\n",
      "Epoch 36/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3579 - accuracy: 0.8772 - val_loss: 0.4896 - val_accuracy: 0.8613\n",
      "\n",
      "Epoch 00036: val_accuracy improved from 0.84540 to 0.86130, saving model to model_save/best_model.hdf5\n",
      "Epoch 37/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3547 - accuracy: 0.8755 - val_loss: 0.5489 - val_accuracy: 0.8487\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.86130\n",
      "Epoch 38/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3553 - accuracy: 0.8768 - val_loss: 0.7942 - val_accuracy: 0.7931\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.86130\n",
      "Epoch 39/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3441 - accuracy: 0.8804 - val_loss: 0.5648 - val_accuracy: 0.8457\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.86130\n",
      "Epoch 40/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3428 - accuracy: 0.8806 - val_loss: 0.7378 - val_accuracy: 0.8127\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.86130\n",
      "Epoch 41/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3374 - accuracy: 0.8827 - val_loss: 0.7922 - val_accuracy: 0.8065\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.86130\n",
      "Epoch 42/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3342 - accuracy: 0.8837 - val_loss: 0.6052 - val_accuracy: 0.8309\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.86130\n",
      "Epoch 43/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3304 - accuracy: 0.8854 - val_loss: 0.5596 - val_accuracy: 0.8519\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.86130\n",
      "Epoch 44/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3256 - accuracy: 0.8853 - val_loss: 0.6472 - val_accuracy: 0.8313\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.86130\n",
      "Epoch 45/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3209 - accuracy: 0.8885 - val_loss: 0.6071 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.86130\n",
      "Epoch 46/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.3177 - accuracy: 0.8884 - val_loss: 0.8330 - val_accuracy: 0.8025\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.86130\n",
      "Epoch 47/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3111 - accuracy: 0.8923 - val_loss: 0.4339 - val_accuracy: 0.8786\n",
      "\n",
      "Epoch 00047: val_accuracy improved from 0.86130 to 0.87860, saving model to model_save/best_model.hdf5\n",
      "Epoch 48/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3119 - accuracy: 0.8914 - val_loss: 0.6324 - val_accuracy: 0.8302\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.87860\n",
      "Epoch 49/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.3055 - accuracy: 0.8914 - val_loss: 0.6880 - val_accuracy: 0.8274\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.87860\n",
      "Epoch 50/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.3073 - accuracy: 0.8919 - val_loss: 0.6586 - val_accuracy: 0.8238\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.87860\n",
      "Epoch 51/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2988 - accuracy: 0.8950 - val_loss: 0.8148 - val_accuracy: 0.8018\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.87860\n",
      "Epoch 52/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2969 - accuracy: 0.8969 - val_loss: 0.5975 - val_accuracy: 0.8461\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.87860\n",
      "Epoch 53/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.2927 - accuracy: 0.8960 - val_loss: 0.4794 - val_accuracy: 0.8586\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.87860\n",
      "Epoch 54/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2885 - accuracy: 0.8993 - val_loss: 0.4936 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.87860\n",
      "Epoch 55/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2823 - accuracy: 0.9010 - val_loss: 0.4693 - val_accuracy: 0.8674\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.87860\n",
      "Epoch 56/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2895 - accuracy: 0.8988 - val_loss: 0.5840 - val_accuracy: 0.8479\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.87860\n",
      "Epoch 57/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2824 - accuracy: 0.9018 - val_loss: 0.6777 - val_accuracy: 0.8317\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.87860\n",
      "Epoch 58/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2786 - accuracy: 0.9009 - val_loss: 0.4826 - val_accuracy: 0.8690\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.87860\n",
      "Epoch 59/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2758 - accuracy: 0.9045 - val_loss: 0.4698 - val_accuracy: 0.8648\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.87860\n",
      "Epoch 60/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2722 - accuracy: 0.9049 - val_loss: 0.6696 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.87860\n",
      "Epoch 61/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2741 - accuracy: 0.9040 - val_loss: 0.6294 - val_accuracy: 0.8438\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.87860\n",
      "Epoch 62/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.2661 - accuracy: 0.9062 - val_loss: 0.4332 - val_accuracy: 0.8822\n",
      "\n",
      "Epoch 00062: val_accuracy improved from 0.87860 to 0.88220, saving model to model_save/best_model.hdf5\n",
      "Epoch 63/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.2672 - accuracy: 0.9074 - val_loss: 0.7264 - val_accuracy: 0.8274\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.88220\n",
      "Epoch 64/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2703 - accuracy: 0.9044 - val_loss: 0.6856 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.88220\n",
      "Epoch 65/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2576 - accuracy: 0.9100 - val_loss: 0.5184 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.88220\n",
      "Epoch 66/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2581 - accuracy: 0.9095 - val_loss: 0.5272 - val_accuracy: 0.8676\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.88220\n",
      "Epoch 67/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2581 - accuracy: 0.9091 - val_loss: 0.5540 - val_accuracy: 0.8581\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.88220\n",
      "Epoch 68/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2556 - accuracy: 0.9102 - val_loss: 0.4755 - val_accuracy: 0.8775\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.88220\n",
      "Epoch 69/300\n",
      "390/390 [==============================] - 54s 138ms/step - loss: 0.2549 - accuracy: 0.9105 - val_loss: 0.6031 - val_accuracy: 0.8508\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.88220\n",
      "Epoch 70/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2519 - accuracy: 0.9114 - val_loss: 0.5857 - val_accuracy: 0.8545\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.88220\n",
      "Epoch 71/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.2502 - accuracy: 0.9118 - val_loss: 0.6660 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.88220\n",
      "Epoch 72/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2488 - accuracy: 0.9120 - val_loss: 0.6014 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.88220\n",
      "Epoch 73/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.2454 - accuracy: 0.9140 - val_loss: 1.0795 - val_accuracy: 0.7798\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.88220\n",
      "Epoch 74/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.2418 - accuracy: 0.9143 - val_loss: 0.5586 - val_accuracy: 0.8578\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.88220\n",
      "Epoch 75/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2446 - accuracy: 0.9143 - val_loss: 0.5407 - val_accuracy: 0.8555\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.88220\n",
      "Epoch 76/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2418 - accuracy: 0.9137 - val_loss: 0.4207 - val_accuracy: 0.8860\n",
      "\n",
      "Epoch 00076: val_accuracy improved from 0.88220 to 0.88600, saving model to model_save/best_model.hdf5\n",
      "Epoch 77/300\n",
      "390/390 [==============================] - 54s 138ms/step - loss: 0.2386 - accuracy: 0.9165 - val_loss: 0.4176 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00077: val_accuracy improved from 0.88600 to 0.88890, saving model to model_save/best_model.hdf5\n",
      "Epoch 78/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2344 - accuracy: 0.9170 - val_loss: 0.5213 - val_accuracy: 0.8676\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.88890\n",
      "Epoch 79/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2375 - accuracy: 0.9159 - val_loss: 0.5751 - val_accuracy: 0.8611\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.88890\n",
      "Epoch 80/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.2287 - accuracy: 0.9195 - val_loss: 0.6035 - val_accuracy: 0.8532\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.88890\n",
      "Epoch 81/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2300 - accuracy: 0.9189 - val_loss: 0.9622 - val_accuracy: 0.7943\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.88890\n",
      "Epoch 82/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.2280 - accuracy: 0.9190 - val_loss: 0.5591 - val_accuracy: 0.8590\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.88890\n",
      "Epoch 83/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2258 - accuracy: 0.9200 - val_loss: 0.4134 - val_accuracy: 0.8852\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.88890\n",
      "Epoch 84/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2254 - accuracy: 0.9206 - val_loss: 0.7240 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.88890\n",
      "Epoch 85/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.2249 - accuracy: 0.9206 - val_loss: 0.4724 - val_accuracy: 0.8774\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.88890\n",
      "Epoch 86/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2230 - accuracy: 0.9215 - val_loss: 0.6941 - val_accuracy: 0.8420\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.88890\n",
      "Epoch 87/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2255 - accuracy: 0.9208 - val_loss: 0.6157 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.88890\n",
      "Epoch 88/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2192 - accuracy: 0.9224 - val_loss: 0.4921 - val_accuracy: 0.8777\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.88890\n",
      "Epoch 89/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2188 - accuracy: 0.9228 - val_loss: 0.4328 - val_accuracy: 0.8864\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.88890\n",
      "Epoch 90/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2206 - accuracy: 0.9222 - val_loss: 0.3978 - val_accuracy: 0.8989\n",
      "\n",
      "Epoch 00090: val_accuracy improved from 0.88890 to 0.89890, saving model to model_save/best_model.hdf5\n",
      "Epoch 91/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2111 - accuracy: 0.9260 - val_loss: 0.4550 - val_accuracy: 0.8818\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.89890\n",
      "Epoch 92/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2169 - accuracy: 0.9235 - val_loss: 0.7731 - val_accuracy: 0.8236\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.89890\n",
      "Epoch 93/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2159 - accuracy: 0.9230 - val_loss: 0.4684 - val_accuracy: 0.8736\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.89890\n",
      "Epoch 94/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2106 - accuracy: 0.9255 - val_loss: 0.5880 - val_accuracy: 0.8531\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.89890\n",
      "Epoch 95/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2110 - accuracy: 0.9261 - val_loss: 0.5677 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.89890\n",
      "Epoch 96/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2145 - accuracy: 0.9241 - val_loss: 0.4401 - val_accuracy: 0.8835\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.89890\n",
      "Epoch 97/300\n",
      "390/390 [==============================] - 54s 138ms/step - loss: 0.2063 - accuracy: 0.9268 - val_loss: 0.4566 - val_accuracy: 0.8834\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.89890\n",
      "Epoch 98/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2058 - accuracy: 0.9273 - val_loss: 0.5670 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.89890\n",
      "Epoch 99/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2035 - accuracy: 0.9280 - val_loss: 0.5963 - val_accuracy: 0.8608\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.89890\n",
      "Epoch 100/300\n",
      "390/390 [==============================] - 54s 138ms/step - loss: 0.2032 - accuracy: 0.9281 - val_loss: 0.5837 - val_accuracy: 0.8532\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.89890\n",
      "Epoch 101/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2043 - accuracy: 0.9283 - val_loss: 0.5397 - val_accuracy: 0.8625\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.89890\n",
      "Epoch 102/300\n",
      "390/390 [==============================] - 55s 142ms/step - loss: 0.2011 - accuracy: 0.9289 - val_loss: 0.5608 - val_accuracy: 0.8612\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.89890\n",
      "Epoch 103/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2017 - accuracy: 0.9284 - val_loss: 0.5711 - val_accuracy: 0.8611\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.89890\n",
      "Epoch 104/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2023 - accuracy: 0.9290 - val_loss: 0.3909 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.89890\n",
      "Epoch 105/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.2019 - accuracy: 0.9278 - val_loss: 0.5110 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.89890\n",
      "Epoch 106/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1974 - accuracy: 0.9305 - val_loss: 0.6048 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.89890\n",
      "Epoch 107/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.2016 - accuracy: 0.9287 - val_loss: 0.4877 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.89890\n",
      "Epoch 108/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.1924 - accuracy: 0.9316 - val_loss: 0.5786 - val_accuracy: 0.8671\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.89890\n",
      "Epoch 109/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.2006 - accuracy: 0.9282 - val_loss: 0.4067 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.89890\n",
      "Epoch 110/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1932 - accuracy: 0.9320 - val_loss: 0.4296 - val_accuracy: 0.8939\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.89890\n",
      "Epoch 111/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.1939 - accuracy: 0.9315 - val_loss: 0.5726 - val_accuracy: 0.8621\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.89890\n",
      "Epoch 112/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1951 - accuracy: 0.9306 - val_loss: 0.5441 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.89890\n",
      "Epoch 113/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1912 - accuracy: 0.9318 - val_loss: 0.4478 - val_accuracy: 0.8937\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.89890\n",
      "Epoch 114/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1902 - accuracy: 0.9336 - val_loss: 0.5353 - val_accuracy: 0.8676\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.89890\n",
      "Epoch 115/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1873 - accuracy: 0.9340 - val_loss: 0.8843 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.89890\n",
      "Epoch 116/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1874 - accuracy: 0.9327 - val_loss: 0.5354 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.89890\n",
      "Epoch 117/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1860 - accuracy: 0.9348 - val_loss: 0.4323 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.89890\n",
      "Epoch 118/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1868 - accuracy: 0.9337 - val_loss: 0.5229 - val_accuracy: 0.8726\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.89890\n",
      "Epoch 119/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1845 - accuracy: 0.9339 - val_loss: 0.5399 - val_accuracy: 0.8677\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.89890\n",
      "Epoch 120/300\n",
      "390/390 [==============================] - 54s 139ms/step - loss: 0.1837 - accuracy: 0.9348 - val_loss: 0.6542 - val_accuracy: 0.8522\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.89890\n",
      "Epoch 121/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1848 - accuracy: 0.9346 - val_loss: 0.5333 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.89890\n",
      "Epoch 122/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1830 - accuracy: 0.9343 - val_loss: 1.2490 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.89890\n",
      "Epoch 123/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1857 - accuracy: 0.9350 - val_loss: 0.4565 - val_accuracy: 0.8863\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.89890\n",
      "Epoch 124/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1806 - accuracy: 0.9364 - val_loss: 0.4847 - val_accuracy: 0.8803\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.89890\n",
      "Epoch 125/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1836 - accuracy: 0.9358 - val_loss: 0.5299 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.89890\n",
      "Epoch 126/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1792 - accuracy: 0.9367 - val_loss: 0.5022 - val_accuracy: 0.8793\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.89890\n",
      "Epoch 127/300\n",
      "390/390 [==============================] - 55s 139ms/step - loss: 0.1766 - accuracy: 0.9391 - val_loss: 0.6804 - val_accuracy: 0.8561\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.89890\n",
      "Epoch 128/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1740 - accuracy: 0.9379 - val_loss: 0.4504 - val_accuracy: 0.8876\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.89890\n",
      "Epoch 129/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1742 - accuracy: 0.9386 - val_loss: 0.5610 - val_accuracy: 0.8708\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.89890\n",
      "Epoch 130/300\n",
      "390/390 [==============================] - 55s 140ms/step - loss: 0.1801 - accuracy: 0.9363 - val_loss: 0.6086 - val_accuracy: 0.8651\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.89890\n",
      "Epoch 131/300\n",
      "390/390 [==============================] - 55s 141ms/step - loss: 0.1726 - accuracy: 0.9387 - val_loss: 0.3589 - val_accuracy: 0.9058\n",
      "\n",
      "Epoch 00131: val_accuracy improved from 0.89890 to 0.90580, saving model to model_save/best_model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96b0286890>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"model_save/best_model.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "monitor = Monitor()\n",
    "\n",
    "model.fit(datagen_train.flow(X_train, y_train, batch_size=batch_size),\n",
    "          validation_data=datagen_test.flow(X_test, y_test, batch_size=batch_size),validation_steps = len(X_test) / batch_size,\n",
    "          steps_per_epoch=len(X_train) / batch_size, epochs=epochs,callbacks=[checkpoint,monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T17:19:50.588451Z",
     "iopub.status.busy": "2022-01-08T17:19:50.588195Z",
     "iopub.status.idle": "2022-01-08T17:19:53.374910Z",
     "shell.execute_reply": "2022-01-08T17:19:53.374162Z",
     "shell.execute_reply.started": "2022-01-08T17:19:50.588422Z"
    },
    "id": "ZcWydmIVhZGr",
    "outputId": "a0345aa5-79ff-4e56-eb94-50437b43c4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 3s 35ms/step - loss: 0.3589 - accuracy: 0.9058\n",
      "Test loss: 0.3589215576648712\n",
      "Test accuracy: 0.9057999849319458\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(datagen_test.flow(X_test, y_test, batch_size=batch_size), steps=len(X_test)/batch_size,verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T17:22:31.290446Z",
     "iopub.status.busy": "2022-01-08T17:22:31.290188Z",
     "iopub.status.idle": "2022-01-08T17:22:31.531141Z",
     "shell.execute_reply": "2022-01-08T17:22:31.530371Z",
     "shell.execute_reply.started": "2022-01-08T17:22:31.290418Z"
    },
    "id": "UE3lF6EH1r_L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
